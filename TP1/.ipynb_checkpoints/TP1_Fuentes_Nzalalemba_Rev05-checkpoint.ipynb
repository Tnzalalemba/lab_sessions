{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 : Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this work is to implement least square linear regression to medical data. The problem is based on an example described in the book by Hastie & Tibshirani (2009) pp. 3-4 & 49-63. Data come from a study published by Stamey et al. (1989). This study aims at the prediction of the level of prostate specific antigen, denoted by `lpsa` below, from the\n",
    "results of clinical exams. These exams are carried out before a possible\n",
    "prostatectomy.\n",
    "\n",
    "The measurements are log cancer volume `lcavol`, log prostate \n",
    "weight `lweight`, age of the patient `age`, log of benign prostatic \n",
    "hyperplasia amount `lbph`, seminal vesicle invasion `svi`, log of capsular \n",
    "penetration `lcp`, Gleason score `gleason`, and percent of Gleason scores 4 or \n",
    "5 `pgg45`. The variables `svi` and `gleason` are categorical, others are\n",
    "quantitative. There are `p=8` entries.\n",
    "The work is decomposed in the following tasks:\n",
    "\n",
    "* read and format the data : extraction of the training and test sets,\n",
    "* apply least square regression method to predict `lpsa` from the entries,\n",
    "* study the estimated error on the test set (validation),\n",
    "* identify the most significant entries by using a rejection test,\n",
    "* apply regularized least square regression method (ridge regression),\n",
    "* search for an optimal regularization parameter thanks to\n",
    "cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "# import os\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Normalize data\n",
    "Data are stored in ASCII format: \n",
    "\n",
    "* the first column enumerates the data from 1 Ã  97 (97 male subjects). \n",
    "* columns 2 to 9 contain the entries themselves. \n",
    "* column 10 contains target values. \n",
    "* column 11 contains label 1 for the training set, \n",
    "and 2 for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% To read data from spaced separated float numbers\n",
    "# x, y = np.loadtxt(c, delimiter=',', usecols=(0, 2), unpack=True)\n",
    "\n",
    "data_init = np.loadtxt('prostate_data_sansheader.txt')\n",
    "\n",
    "data = data_init[:,1:]   # we get rid of the indices (1 to 97)\n",
    "\n",
    "#%% Extraction of training/test sets\n",
    "Itrain = np.nonzero(data[:,-1]==1)\n",
    "data_train=data[Itrain]   # original data\n",
    "\n",
    "Itest = np.nonzero(data[:,-1]==0)\n",
    "data_test = data[Itest]   # original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization of the data** *with respect to the mean and standard deviation of the training set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.23328245   0.47303067   7.44601122   1.45269103   0.41684299\n",
      "   1.39024269   0.70355366  29.08227243]\n"
     ]
    }
   ],
   "source": [
    "M_train = data_train\n",
    "M_test = data_test \n",
    "moy = np.zeros((8,))\n",
    "sigma = np.zeros((8,))\n",
    "\n",
    "# With a FOR loop :\n",
    "for k in range(8): # 8 columns of entries\n",
    "    moy[k]=np.mean(data_train[:,k])\n",
    "    sigma[k] = np.std(data_train[:,k], ddof=0)\n",
    "    M_train[:,k] = (data_train[:,k]-moy[k])/sigma[k] # normalized: centered, variance 1\n",
    "    M_test[:,k] = (data_test[:,k]-moy[k])/sigma[k]   # same normalization for test set\n",
    "\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternative WITHOUT FOR\n",
    "normalize = lambda vec: (vec-np.mean(vec))/np.std(vec)    # inline function \n",
    "M_train = np.array( [ normalize(vec) for vec in data_train[:,0:8].T ] ).T  # iterate on vec direct / ARRAY not LIST\n",
    "moy = np.array( [ np.mean(vec) for vec in data_train[:,0:8].T ] )\n",
    "sigma = np.array( [ np.std(vec, ddof=0) for vec in data_train[:,0:8].T ] )\n",
    "\n",
    "M_test = np.array([ (data_test[:,k]-moy[k])/sigma[k] for k in range(M_train.shape[1]) ] ).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : simple least square regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary questions\n",
    " \n",
    " * Compute the autocovariance matrix from the training set.\n",
    " * Observe carefully & Comment. What kind of information can you get ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.30023199  0.28632427  0.06316772  0.59294913  0.69204308\n",
      "   0.42641407  0.48316136]\n",
      " [ 0.30023199  1.          0.31672347  0.43704154  0.18105448  0.15682859\n",
      "   0.02355821  0.07416632]\n",
      " [ 0.28632427  0.31672347  1.          0.28734645  0.12890226  0.1729514\n",
      "   0.36591512  0.27580573]\n",
      " [ 0.06316772  0.43704154  0.28734645  1.         -0.1391468  -0.08853456\n",
      "   0.03299215 -0.03040382]\n",
      " [ 0.59294913  0.18105448  0.12890226 -0.1391468   1.          0.67124021\n",
      "   0.30687537  0.48135774]\n",
      " [ 0.69204308  0.15682859  0.1729514  -0.08853456  0.67124021  1.\n",
      "   0.47643684  0.66253335]\n",
      " [ 0.42641407  0.02355821  0.36591512  0.03299215  0.30687537  0.47643684\n",
      "   1.          0.7570565 ]\n",
      " [ 0.48316136  0.07416632  0.27580573 -0.03040382  0.48135774  0.66253335\n",
      "   0.7570565   1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe autocovariance matrix is symmetric.\\nWe can notice the the diagonal is giving us the varaiance (the square of the standard deviation) of features and\\nof the target which is the last value of the diagonal. The rest of the covariance matrix is telling about the\\ncovariance between the features (each other) and between the features and the target.\\nThe covariances values are non-zero. Thus, the features are varying together, as well with the target.\\nWe can also notice that, since the features have been normalized, their variances (in the diagonal) are equal to 1, which is\\nthe square of their standard deviation.\\n\\nThe 1st feature has the strongest relationship with the target where the covariance is 0.879.\\nThe 3rd feature has the weakest relationship with the target where the covariance is 0.273.\\n\\nThe 1st feature has a weak relationship with the 4th feature where the covariance is 0.063.\\nThe 2nd feature has a weak relationship with the 7th feature where the covariance is 0.023.\\nThe 4th feature has a weak relationship with the 8th feature where the covariance is-0.030,\\nand they are not varying in the same direction since their covariance is negative.\\n\\nThe 1st feature has a strong relationship with the 6th feature where the covariance is 0.692.\\nThe 5th feature has a strong relationship with the 6th feature where the covariance is 0.671.\\nThe 7th feature has a strong relationship with the 8th feature where the covariance is 0.757.\\nThe 8th feature has a strong relationship with the 6th feature where the covariance is 0.662.\\n\\nThe 4th and the 5th features are stongly varying in opposite directions since the record the lowest covariance value\\nwhich is -0.139\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preliminary questions\n",
    "\n",
    "# We are computing the autocovariance matrix of the trainning dataset containing the information about the features and the target.\n",
    "cov_matrix = np.cov(M_train[:,0:9].T, ddof=0) # ddof = 0 to force the computation using the simple average\n",
    "\n",
    "print(cov_matrix)\n",
    "\n",
    "\"\"\"\n",
    "We can also compute the autocovariance matrix of training set with features only with the below exprrssion:\n",
    "cov_matrix = np.cov(M_train[:,0:8].T, ddof=0)\n",
    "\n",
    "However, we prefer computing the autocovariance of the training set with target involved in order to have an\n",
    "insight of how the features are relates to the target.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Comments\n",
    "\"\"\"\n",
    "The autocovariance matrix is symmetric.\n",
    "We can notice the the diagonal is giving us the varaiance (the square of the standard deviation) of features and\n",
    "of the target which is the last value of the diagonal. The rest of the covariance matrix is telling about the\n",
    "covariance between the features (each other) and between the features and the target.\n",
    "The covariances values are non-zero. Thus, the features are varying together, as well with the target.\n",
    "We can also notice that, since the features have been normalized, their variances (in the diagonal) are equal to 1, which is\n",
    "the square of their standard deviation.\n",
    "\n",
    "The 1st feature has the strongest relationship with the target where the covariance is 0.879.\n",
    "The 3rd feature has the weakest relationship with the target where the covariance is 0.273.\n",
    "\n",
    "The 1st feature has a weak relationship with the 4th feature where the covariance is 0.063.\n",
    "The 2nd feature has a weak relationship with the 7th feature where the covariance is 0.023.\n",
    "The 4th feature has a weak relationship with the 8th feature where the covariance is-0.030,\n",
    "and they are not varying in the same direction since their covariance is negative.\n",
    "\n",
    "The 1st feature has a strong relationship with the 6th feature where the covariance is 0.692.\n",
    "The 5th feature has a strong relationship with the 6th feature where the covariance is 0.671.\n",
    "The 7th feature has a strong relationship with the 8th feature where the covariance is 0.757.\n",
    "The 8th feature has a strong relationship with the 6th feature where the covariance is 0.662.\n",
    "\n",
    "The 4th and the 5th features are stongly varying in opposite directions since the record the lowest covariance value\n",
    "which is -0.139\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques rappels Ã  propos de la notion de covariance : https://fr.wikipedia.org/wiki/Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : least square regression \n",
    " * Build the matrix of features `X_train` for the training set, the first column is made of ones.\n",
    " * Estimate the regression vector `beta_hat` (estimates= `X*beta_hat`)\n",
    " _Indication: you may either use the function `inv` or another more efficient way to compute $A^{-1}B$ (think of `A\\B`)._ \n",
    " * What is the value of the first coefficient `beta_hat[0]` ? What does it correspond to ?\n",
    " * Estimate the prediction error (quadratic error) from the test set.\n",
    "\n",
    "\n",
    "*Indication: be careful of using `X_test` defined above, normalized w.r.t. the training data set. You can estimate this error by using:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_test = data_test[:,8]   # target column\n",
    "N_test = data_test.shape[0]\n",
    "X_test = np.concatenate((np.ones((N_test,1)), M_test[:,0:8]), axis=1) \n",
    "# don't forget the 1st column of ones and normalization !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "\n",
    "# Build the matrix of features X_train\n",
    "t_train = data_train[:,8]   # target column\n",
    "N_train = data_train.shape[0]\n",
    "X_train = np.concatenate((np.ones((N_train,1)), M_train[:,0:8]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression vector beta_hat is [ 2.45234509  0.71104059  0.29045029 -0.14148182  0.21041951  0.30730025\n",
      " -0.28684075 -0.02075686  0.27526843]\n"
     ]
    }
   ],
   "source": [
    "# Estimate the regression vector beta_hat (estimates= X*beta_hat)\n",
    "\n",
    "beta_hat =np.matmul(inv(np.matmul(X_train.T,(X_train))),np.matmul(X_train.T,t_train))\n",
    "print(\"The regression vector beta_hat is \"+str(beta_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the first coefficient of beta_hat is 2.45234508507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThis value corresponds to the intercept or the bias. It is a constant value that represent the mean value of the target.\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the value of the first coefficient beta_hat[0] ? What does it correspond to ?\n",
    "print(\"The value of the first coefficient of beta_hat is \"+str(beta_hat[0]))\n",
    "\n",
    "\"\"\"\n",
    "This value corresponds to the intercept or the bias. It is a constant value that represent the mean value of the target.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction error is 0.521274005508\n"
     ]
    }
   ],
   "source": [
    "# Estimate the prediction error (quadratic error) from the test set.\n",
    "estimates = np.matmul(X_test,beta_hat)\n",
    "\n",
    "# Calculation of Mean Squared Error (MSE)\n",
    "MSE = np.mean(np.square(t_test - estimates))\n",
    "\n",
    "print(\"The prediction error is \"+str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection test, computation of Z-scores\n",
    "Now we turn to the selection of the most significant entries so that our predictor be more robust. The essential idea is that our estimates will be more robust if only the most significant entries are taken into account. As a consequence, note that we will *reduce the dimension* of the problem from |p=8| to some smaller dimension. The present approach uses a statistical test to decide whether the regression coefficient corresponding to some entry is significantly non-zero. Then we can decide either to put non significant coefficients to zero, or to select the significant entries only and estimate the new reduced regression vector.\n",
    "\n",
    "Let's assume that target values are noisy due to some white Gaussian\n",
    "noise with variance $\\sigma^2$ (see Hastie & Tibshirani p. 47). One can show that the estimated regression vector |beta_hat| is also Gaussian with variance\n",
    "\n",
    "$$ var (\\widehat{\\beta}) = (X^TX)^{-1}\\sigma^2.$$  \n",
    "\n",
    "One can also show that the estimator of the variance (from the training set)\n",
    "\n",
    "$$\\widehat{\\sigma^2}=\\frac{1}{(N-p-1)}\\sum (t_n-\\widehat{t}_n)^2$$\n",
    "\n",
    "obeys a Chi-2 distribution. As a consequence a Chi-square statistical test can be used to determine whether some coefficient $\\beta_j$ is\n",
    "significantly non-zero. To this aim, one defines the variables $z_j$\n",
    "named Z-scores which in turn obey a Fisher law, also called\n",
    "$t$-distribution, which are often used in statistics:\n",
    "\n",
    "$$ z_j = \\frac{\\beta_j}{\\widehat{\\sigma}\\sqrt{v_j}} $$\n",
    "\n",
    "where $v_j$ is the $j$-th diagonal element of the matrix $(X^TX)^{-1}$.\n",
    "For sake of simplicity, we will consider that the null hypothesis of\n",
    "$\\beta_j$ is rejected with probability 95% if the Z-score is greater than 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "1. Compute the Z-scores and select the most significant entries.\n",
    "2. Estimate the prediction error over the test set if only these significant \n",
    "entries are taken into account for regression by putting other regression \n",
    "coefficients to zero.\n",
    "3. Estimate the new regression vector when only the significant features\n",
    "are taken into account.\n",
    "4. Compare to previous results (Exercise 1).\n",
    "\n",
    "*Indication 1 : to sort a vector `Z` in descending order*\n",
    "`val = np.sort(np.abs(Z))[-1:0:-1]`\n",
    "\n",
    "\n",
    "*Indication 2 :* to extract the diagonal of a matrix,\n",
    "`vXX = np.diag(inv(X.T.dot(X),k=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "\n",
    "# Compute the Z-scores and select the most significant entries.\n",
    "\n",
    "Sigma_square = np.std(t_train, ddof=0) # ddof = 0 to force the computation using the simple average\n",
    "var_beta_hat = inv(np.matmul(X_train.T,X_train))*np.square(Sigma_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_hat_train = np.matmul(X_train,beta_hat) #Computed on the training set\n",
    "p = 8 # the number of features\n",
    "Sigma_square_hat = 1/(N_train-p-1)*np.sum(np.square(t_train-t_hat_train)) \n",
    "sigma_hat = np.sqrt(Sigma_square_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vXX = np.diag(inv(X_train.T.dot(X_train)),k=0) # Extraction of the diagonal of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_scores = beta_hat/(sigma_hat*np.sqrt(vXX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Z-score is given by [ 28.18152744   5.36629046   2.75078939   1.39590898   2.05584563\n",
      "   2.46925518   1.86691264   0.14668121   1.73783972]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTherefore, we can select the 1st, the 2nd, the 4th and the 5th features + the intercept for predictions.\\nBecause their Z-scores are higher than 2.\\nThis is a direct consequense of the following assumption: \"the null hypothesis of Beta_j is rejected with probanility of\\n95% if the Z-score is greater than 2.\" \\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will consider that the null hypothesis of  Î²j  is rejected with probability 95% if the Z-score is greater than 2.\n",
    "print(\"The Z-score is given by \"+str(np.abs(Z_scores)))\n",
    "\n",
    "\"\"\"\n",
    "Therefore, we can select the 1st, the 2nd, the 4th and the 5th features + the intercept for predictions.\n",
    "Because their Z-scores are higher than 2.\n",
    "This is a direct consequense of the following assumption: \"the null hypothesis of Beta_j is rejected with probanility of\n",
    "95% if the Z-score is greater than 2.\" \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the prediction error over the test set if only these significant entries are taken into account for regression by putting other regression coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new regression vector beta_hat_new is [ 2.45234509  0.71104059  0.29045029  0.          0.21041951  0.30730025\n",
      "  0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# New beta_hat\n",
    "beta_hat_new = np.array([beta_hat[0], beta_hat[1], beta_hat[2], 0, beta_hat[4], beta_hat[5], 0, 0, 0])\n",
    "print(\"The new regression vector beta_hat_new is \"+str(beta_hat_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimate the new regression vector when only the significant features are taken into account.\n",
    "estimates_new = np.matmul(X_test,beta_hat_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new prediction error is 0.452226616073\n"
     ]
    }
   ],
   "source": [
    "# Computation of the new Mean Squared Error (MSE_new)\n",
    "MSE_new = np.mean(np.square(t_test - estimates_new))\n",
    "\n",
    "print(\"The new prediction error is \"+str(MSE_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Exercise 1, the MSE is 0.521274005508\n",
      "In Exercise 2, after feature selection, the MSE is 0.452226616073, which is smaller the one of Exercise 1\n",
      "Therefore, one can say that feature selection improved the prediction error! And the predictor is more robust !\n"
     ]
    }
   ],
   "source": [
    "# Compare to previous results (Exercise 1).\n",
    "print(\"In the Exercise 1, the MSE is \"+str(MSE))\n",
    "print(\"In Exercise 2, after feature selection, the MSE is \"+str(MSE_new)+\", which is smaller the one of Exercise 1\")\n",
    "print(\"Therefore, one can say that feature selection improved the prediction error! And the predictor is more robust !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regularized least squares\n",
    "This part deals with regularized least square regression. We denote\n",
    "by `beta_hat_reg` the resulting coefficients. This approach is an alternative to the selection based on statistical tests above. The idea is now to penalize large values of regression coefficients, *except for the bias*.\n",
    "\n",
    "We use the result:\n",
    "\n",
    "$$\\hat{\\beta} = (\\lambda I_p + X_c^T X_c)^{-1} X_c^T t_c$$\n",
    "\n",
    "where $X_c$ contains the normalized entries of the training data set with \n",
    "no column of ones (the bias should no be penalized and is processed). \n",
    "The targets `t_c` are therefore also centered, `t_c=t-mean(t)`.\n",
    " \n",
    "First, we estimate the bias $t_0$ to center the targets which yields the coefficient $\\beta_0$, that is `beta_hat_reg[0]` in Python.\n",
    "\n",
    "*Remark : the bias is estimated as the empirical average of targets.\n",
    "For tests, entries should be normalized with respect to the means and\n",
    "variances of the training data set (see exercise 3.5 p. 95 in Hastie & Tibshirani). Then work on the vector of entries with no column of ones.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "1. Use _ridge regression_ for penalty `lambda = 25` to estimate the regression vector. \n",
    "2. Estimate the prediction error from the test set.\n",
    "3. Compare the results (coefficients $\\beta$, error...) to previous ones.\n",
    "4. You may also compare these results to the result of best subset selection below:\n",
    "\n",
    "`beta_best = [2.477 0.74 0.316 0 0 0 0 0 0]`.\n",
    "\n",
    "*Indication : a simple way to obtain predictions for the test data set is the code below:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nt = data_train[:,8]   # column of targets\\nt0 = np.mean(t)\\n\\nN_test = data_test.shape[0]\\nX_test = np.hstack((np.ones((N_test,1)), M_test[:,0:8]))  \\n# Here the 1st column of X_test is a column of ones.\\nt_hat_reg = X_test.dot(beta_hat_reg)\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "t = data_train[:,8]   # column of targets\n",
    "t0 = np.mean(t)\n",
    "\n",
    "N_test = data_test.shape[0]\n",
    "X_test = np.hstack((np.ones((N_test,1)), M_test[:,0:8]))  \n",
    "# Here the 1st column of X_test is a column of ones.\n",
    "t_hat_reg = X_test.dot(beta_hat_reg)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "# Center the target\n",
    "t = data_train[:,8]   # column of targets\n",
    "t0 = np.mean(t) # The mean of the targets. This value is the same as beta_hat[0]\n",
    "tc = t - t0 # Center the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the features: The features where already normalize within M_train\n",
    "Xc_train = M_train[:,0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lambda = 25 # The regularization term\n",
    "\n",
    "beta_hat_rg = inv(Lambda*np.identity(p) + Xc_train.T.dot(Xc_train)).dot(Xc_train.T).dot(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ridge regression vector is [ 2.45234509  0.4221092   0.24879171 -0.04226499  0.16575364  0.23091485\n",
      "  0.01066329  0.04306017  0.13151316]\n",
      "The regression vector of Exercise 1 is [ 2.45234509  0.71104059  0.29045029 -0.14148182  0.21041951  0.30730025\n",
      " -0.28684075 -0.02075686  0.27526843]\n",
      "The regression vector of Exercise 2 is [ 2.45234509  0.71104059  0.29045029  0.          0.21041951  0.30730025\n",
      "  0.          0.          0.        ]\n",
      "The main common point between the above regression vectors is the intercept/the bias which is the same !\n",
      "We can notice as well that some coefficients increased and other decreased.\n"
     ]
    }
   ],
   "source": [
    "# The ridge regression vector is therefore:\n",
    "\n",
    "beta_hat_reg = np.hstack((beta_hat[0], beta_hat_rg[0:8]))\n",
    "\n",
    "print(\"The ridge regression vector is \"+str(beta_hat_reg))\n",
    "print(\"The regression vector of Exercise 1 is \"+str(beta_hat))\n",
    "print(\"The regression vector of Exercise 2 is \"+str(beta_hat_new))\n",
    "\n",
    "print(\"The main common point between the above regression vectors is the intercept/the bias which is the same !\")\n",
    "print(\"We can notice as well that some coefficients increased and other decreased.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction error in ridge regression is 0.493942159912.\n",
      "The error in ridge regression is lower than in regular regression (Exercise 1), and higher than the one after features selection in Exercise 2.\n"
     ]
    }
   ],
   "source": [
    "# Estimate the prediction error from the test set.\n",
    "\n",
    "# Estimation of the prediction over test data\n",
    "N_test = data_test.shape[0]\n",
    "Xc_test = np.hstack((np.ones((N_test,1)), M_test[:,0:8])) # From M_test, the test data are already normalized.\n",
    "\n",
    "t_hat_reg_test = X_test.dot(beta_hat_reg)\n",
    "\n",
    "# Computation of the Mean Squared Error in ridge regression (MSE_reg)\n",
    "MSE_reg = np.mean(np.square(t_test - t_hat_reg_test))\n",
    "\n",
    "print(\"The prediction error in ridge regression is \"+str(MSE_reg)+\".\")\n",
    "print(\"The error in ridge regression is lower than in regular regression (Exercise 1), and higher than the one after features selection in Exercise 2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction error with the best regression vector is 0.493942159912.\n"
     ]
    }
   ],
   "source": [
    "# Estimation of the prediction error over test data with the best regression vector\n",
    "beta_best = np.array([ 2.477,  0.74 ,  0.316, 0.0,  0.0, 0.0,  0.0,  0.0,  0.0])\n",
    "\n",
    "t_hat_best_test = Xc_test.dot(beta_hat_reg)\n",
    "MSE_best = np.mean(np.square(t_test - t_hat_best_test))\n",
    "\n",
    "print(\"The prediction error with the best regression vector is \"+str(MSE_best)+\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_best-MSE_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears that the prediction with the best regession vector gives the same mean squared error as the ridge regression vector.\n"
     ]
    }
   ],
   "source": [
    "# You may also compare these results to the result of best subset selection\n",
    "print(\"It appears that the prediction with the best regession vector gives the same mean squared error as the ridge regression vector.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Cross-Validation \n",
    "\n",
    "## How to choose lambda from the training data set only ? \n",
    "\n",
    "The idea is to decompose the training set in 2 subsets: one subset for\n",
    "linear regression (say 9/10 of the data), the other to estimate the prediction error (say 1/10 of the data).\n",
    "\n",
    "We can repeat this operation 10 times over the 10 possible couples of\n",
    "subsets to estimate the average prediction error. We will choose the\n",
    "value of `lambda` which minimizes this error. The algorithm goes as\n",
    "follows:\n",
    "\n",
    "For the 10 cross-validation cases\n",
    "    \n",
    "    Extraction of test & training subsets `testset` & `trainset`\n",
    "    \n",
    "    For lambda in 0:40\n",
    "        Estimate `beta_hat` from normalized `trainset` (mean=0, var=1)\n",
    "        Estimate the error from  `testset`\n",
    "    EndFor lambda\n",
    "\n",
    "EndFor 10 cases\n",
    "\n",
    "Compute the average error for each lambda\n",
    "\n",
    "Choose `lambda` which minimizes the error \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "* Use 6-fold cross-validation in the present study to optimize the choice of `lambda`. \n",
    "Try values of `lambda` ranging from 0 to 40 for instance (0:40).\n",
    "* Plot the estimated error as a function of `lambda`.\n",
    "* Propose a well chosen value of `lambda` and give the estimated corresponding\n",
    "error on the test set.\n",
    "* Comment on your results.\n",
    "\n",
    "*Indication 1 : think of shuffling the dataset first.*\n",
    "\n",
    "*Indication 2 : you can build 6 training and test subsets by using the code below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmax = 40\n",
    "lambda_pos = arange(0,lmax+1) \n",
    "\n",
    "N_test = 10\n",
    "m=np.zeros(8)\n",
    "s = np.zeros(8)\n",
    "X_traink = np.zeros((X_train.shape[0]-N_test,8))\n",
    "X_testk = np.zeros((N_test,8))\n",
    "erreur = np.zeros((6,lmax+1))\n",
    "erreur_rel = np.zeros((6,lmax+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.16563155  2.17915086  2.19518172  2.21256979  2.23072365  2.2493077\n",
      "   2.26811733  2.28702112  2.30593148  2.3247884   2.34355002  2.36218682\n",
      "   2.38067789  2.39900847  2.41716826  2.43515026  2.45294992  2.47056455\n",
      "   2.48799285  2.50523458  2.52229031  2.53916123  2.55584899  2.5723556\n",
      "   2.58868333  2.60483464  2.62081214  2.63661852  2.65225654  2.66772897\n",
      "   2.68303864  2.69818832  2.71318081  2.72801885  2.74270515  2.75724239\n",
      "   2.77163321  2.78588016  2.79998579  2.81395257  2.82778291]\n",
      " [ 2.04886609  2.03006236  2.01460943  2.00154728  1.99031785  1.98055667\n",
      "   1.97200671  1.96447689  1.95781973  1.95191818  1.94667738  1.94201908\n",
      "   1.93787791  1.93419859  1.93093392  1.92804329  1.92549144  1.92324763\n",
      "   1.92128483  1.9195792   1.91810958  1.91685711  1.91580492  1.91493785\n",
      "   1.91424223  1.91370572  1.9133171   1.91306616  1.9129436   1.91294091\n",
      "   1.91305027  1.91326451  1.91357702  1.91398171  1.91447293  1.91504545\n",
      "   1.91569444  1.91641538  1.91720408  1.91805663  1.91896936]\n",
      " [ 1.83712273  1.83031291  1.82895772  1.83075203  1.83448366  1.83945143\n",
      "   1.84522242  1.85151649  1.858146    1.86498193  1.87193389  1.87893765\n",
      "   1.88594715  1.89292919  1.89985984  1.90672194  1.91350329  1.92019538\n",
      "   1.92679251  1.93329102  1.93968884  1.94598509  1.95217977  1.95827356\n",
      "   1.96426761  1.97016349  1.97596298  1.98166809  1.98728094  1.99280373\n",
      "   1.9982387   2.00358812  2.00885425  2.01403931  2.01914551  2.024175\n",
      "   2.0291299   2.03401224  2.03882403  2.0435672   2.04824361]\n",
      " [ 1.58869719  1.59934475  1.60852638  1.61630426  1.62286661  1.62839414\n",
      "   1.63303959  1.63692899  1.64016644  1.64283855  1.64501806  1.64676656\n",
      "   1.64813667  1.64917358  1.64991639  1.65039906  1.65065128  1.65069905\n",
      "   1.65056529  1.65027019  1.64983167  1.64926561  1.64858616  1.64780592\n",
      "   1.64693618  1.64598701  1.64496748  1.6438857   1.64274898  1.64156391\n",
      "   1.64033642  1.63907186  1.63777507  1.63645043  1.63510189  1.63373303\n",
      "   1.63234711  1.63094705  1.62953553  1.62811497  1.62668755]\n",
      " [ 1.21708567  1.20163752  1.19121555  1.18369622  1.17812336  1.17397187\n",
      "   1.17091264  1.16872042  1.16723167  1.16632271  1.16589725  1.16587872\n",
      "   1.16620523  1.16682608  1.1676993   1.16878985  1.17006824  1.17150948\n",
      "   1.17309228  1.17479839  1.17661211  1.17851985  1.18050981  1.18257172\n",
      "   1.18469659  1.18687652  1.18910459  1.19137465  1.19368129  1.1960197\n",
      "   1.19838559  1.20077517  1.20318502  1.20561209  1.20805367  1.2105073\n",
      "   1.21297078  1.21544212  1.21791953  1.2204014   1.22288627]\n",
      " [ 1.33709247  1.30094729  1.26763258  1.23711157  1.20915368  1.1834979\n",
      "   1.1598975   1.13813158  1.11800617  1.0993521   1.08202195  1.0658871\n",
      "   1.05083502  1.0367669   1.02359572  1.01124452  0.99964502  0.98873643\n",
      "   0.97846442  0.96878033  0.9596404   0.95100517  0.94283897  0.93510947\n",
      "   0.92778728  0.92084562  0.91426005  0.90800819  0.90206952  0.89642519\n",
      "   0.89105785  0.88595148  0.88109128  0.87646356  0.87205564  0.86785573\n",
      "   0.86385286  0.86003683  0.85639814  0.85292789  0.84961778]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4 \n",
    "\n",
    "# Shuffling the dataset first\n",
    "import random\n",
    "random.shuffle(data_train) # Randomly shuffle the arrays of the dataset\n",
    "\n",
    "for p in range(6):   # loop on test subsets\n",
    "    # extraction of testset\n",
    "    testset  = data_train[arange(p*N_test,(p+1)*N_test),0:9] \n",
    "    # extraction of trainset\n",
    "    trainset = data_train[hstack((arange(p*N_test),arange((p+1)*N_test,data_train.shape[0]))),0:9]\n",
    "    # Center the target\n",
    "    target_train = trainset[:,8]  # column of targets\n",
    "    target0 = np.mean(target_train) # The mean of the targets.\n",
    "    targetc = target_train - target0 # The targets of the train set are centered\n",
    "    # normalization of entries, features\n",
    "    for i in range(8): # 8 columns of entries\n",
    "        m[i]=np.mean(trainset[:,i])\n",
    "        s[i] = np.std(trainset[:,i], ddof=0)\n",
    "        X_traink[:,i] = (trainset[:,i]-m[i])/s[i] # normalized: centered, variance 1\n",
    "        X_testk[:,i] = (testset[:,i]-m[i])/s[i]   # same normalization for test set\n",
    "    for lambda_p in range(lmax+1): # 40 options of lambda\n",
    "        beta_hat_cv = inv(lambda_p*np.identity(8) + X_traink.T.dot(X_traink)).dot(X_traink.T).dot(targetc)\n",
    "        X_test_cv = np.hstack((np.ones((N_test,1)), X_testk[:,0:8]))\n",
    "        \n",
    "        beta_hat_reg_cv = np.hstack((beta_hat[0], beta_hat_cv[0:8]))\n",
    "        t_hat_cv_test = X_test_cv.dot(beta_hat_reg_cv)\n",
    "        # Computation of the Mean Squared Error in ridge regression (erreur)\n",
    "        erreur[p,lambda_p] = np.mean(np.square(testset[:,8] - t_hat_cv_test))\n",
    "print(erreur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.69908262  1.69024262  1.6843539   1.68033019  1.67761147  1.67586329\n",
      "  1.67486603  1.67446592  1.67455025  1.67503365  1.67584976  1.67694599\n",
      "  1.67827998  1.67981714  1.68152891  1.68339149  1.68538487  1.68749209\n",
      "  1.6896987   1.69199229  1.69436215  1.69679901  1.69929477  1.70184235\n",
      "  1.70443554  1.70706883  1.70973739  1.71243688  1.71516348  1.71791373\n",
      "  1.72068458  1.72347324  1.72627724  1.72909433  1.73192247  1.73475982\n",
      "  1.73760472  1.74045563  1.74331118  1.74617011  1.74903125] [ 0.34990816  0.35885852  0.36822003  0.37777275  0.38735542  0.39686472\n",
      "  0.40623721  0.41543535  0.42443825  0.43323574  0.4418246   0.45020604\n",
      "  0.4583841   0.46636454  0.47415412  0.48176016  0.48919016  0.49645162\n",
      "  0.50355188  0.51049807  0.51729702  0.52395524  0.5304789   0.53687385\n",
      "  0.54314562  0.5492994   0.55534011  0.56127235  0.56710046  0.57282854\n",
      "  0.57846042  0.58399973  0.58944989  0.59481411  0.60009542  0.60529669\n",
      "  0.61042063  0.61546979  0.62044659  0.62535334  0.6301922 ]\n",
      "The relative error for each lambda value is [[ 27.4588729   28.92532955  30.32782031  31.67470297  32.97021919\n",
      "   34.2178517   35.42082065  36.58212432  37.70452584  38.7905495\n",
      "   39.84248964  40.86242714  41.85224897  42.8136682   43.748243\n",
      "   44.65739407  45.54242016  46.40451185  47.24476366  48.06418451\n",
      "   48.86370702  49.64419546  50.40645282  51.15122692  51.8792158\n",
      "   52.59107242  53.28740886  53.96879998  54.63578664  55.28887868\n",
      "   55.92855739  56.55527788  57.16947114  57.77154583  58.36188999\n",
      "   58.94087255  59.50884463  60.06614084  60.61308036  61.14996793\n",
      "   61.67709485]\n",
      " [ 20.58660778  20.10479082  19.60725326  19.11630749  18.63997619\n",
      "   18.18127932  17.74116112  17.31961051  16.91615289  16.53008824\n",
      "   16.16061455  15.80689483  15.46809439  15.14340151  14.83203869\n",
      "   14.53326824  14.24639455  13.97076422  13.70576512  13.45082462\n",
      "   13.20540756  12.96901393  12.74117662  12.52145914  12.30945342\n",
      "   12.10477776  11.90707486  11.71601007  11.53126961  11.35255911\n",
      "   11.17960211  11.01213878  10.84992467  10.69272963  10.54033678\n",
      "   10.39254152  10.24915074  10.10998198   9.9748627    9.84362962\n",
      "    9.71612808]\n",
      " [  8.12439067   8.28699365   8.58512116   8.95192183   9.35092542\n",
      "    9.76142558  10.17134418  10.5735551   10.96388415  11.33996847\n",
      "   11.70057974  12.04520975  12.37380956  12.68662211  12.98407283\n",
      "   13.26669742  13.53509373  13.78988957  14.03172128  14.26121947\n",
      "   14.47899982  14.68565718  14.88176209  15.06785887  15.24446487\n",
      "   15.41207042  15.57113933  15.72210972  15.865395    16.00138509\n",
      "   16.13044754  16.25292883  16.36915553  16.47943551  16.58405907\n",
      "   16.68330001  16.77741668  16.86665296  16.95123913  17.03139278\n",
      "   17.10731958]\n",
      " [  6.49676634   5.37779989   4.50187528   3.81031864   3.26326212\n",
      "    2.83251914   2.49730091   2.24172522   2.05331599   1.92205661\n",
      "    1.83976528   1.79966595   1.79608331   1.82421953   1.8799867\n",
      "    1.95987846   2.06086988   2.1803382    2.31599927   2.46585595\n",
      "    2.62815594   2.80135688   2.98409729   3.17517232   3.37351318\n",
      "    3.57816978   3.78829593   4.00313665   4.22201729   4.44433412\n",
      "    4.66954617   4.89716818   5.12676444   5.35794334   5.59035271\n",
      "    5.82367562   6.05762673   6.29194903   6.52641102   6.7608041\n",
      "    6.99494038]\n",
      " [ 28.36807005  28.90739428  29.27759713  29.5557369   29.77376564\n",
      "   29.94823129  30.08917613  30.20339142  30.29581101  30.37019234\n",
      "   30.42948861  30.47607209  30.51187849  30.53850605  30.55728656\n",
      "   30.56933813  30.57560532  30.57689039  30.57387789  30.56715437\n",
      "   30.5572243   30.54452299  30.52942722  30.51226402  30.493318\n",
      "   30.47283736  30.4510391   30.42811324  30.4042265   30.37952539\n",
      "   30.35413886  30.32818054  30.30175067  30.27493777  30.24782007\n",
      "   30.22046678  30.19293913  30.16529131  30.13757133  30.10982166\n",
      "   30.08207991]\n",
      " [ 21.30503495  23.03191986  24.74072232  26.37687674  27.92409303\n",
      "   29.37980616  30.74684891  32.03017328  33.2354359   34.36835726\n",
      "   35.43443004  36.43879368  37.38619113  38.28096624  39.12708126\n",
      "   39.92814315  40.68743324  41.40793706  42.0923729   42.74321828\n",
      "   43.36273415  43.9529867   44.51586702  45.05310859  45.56630292\n",
      "   46.05691346  46.52628803  46.97566987  47.40620747  47.81896337\n",
      "   48.21492201  48.59499676  48.96003623  49.31082987  49.64811305\n",
      "   49.97257167  50.2848462   50.58553544  50.87519984  51.15436457\n",
      "   51.42352223]]\n",
      "The averaged relative error for each lambda value over the 6 training/test sets is [ 18.72329045  19.10570468  19.50673158  19.91431076  20.3203736\n",
      "  20.72018553  21.11110865  21.49176331  21.86152096  22.22020207\n",
      "  22.56789464  22.90484391  23.23138431  23.54789727  23.85478484\n",
      "  24.15245325  24.44130281  24.72172188  24.99408335  25.25874287\n",
      "  25.51603813  25.76628886  26.00979718  26.24684831  26.47771136\n",
      "  26.7026402   26.92187435  27.13563992  27.34415042  27.54760762\n",
      "  27.74620235  27.94011516  28.12951711  28.31457032  28.49542861\n",
      "  28.67223803  28.84513735  29.01425859  29.17972739  29.34166344\n",
      "  29.50018084]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4 ---------------\n",
    "# ...\n",
    "# Averaged error on the 6 training/test sets on each lambda ?\n",
    "erreur_lambda=np.zeros(lmax+1)\n",
    "std_erreur_lambda=np.zeros(lmax+1)\n",
    "\n",
    "for j in range(lmax+1):\n",
    "    erreur_lambda[j] = np.mean(erreur[:,j])\n",
    "    std_erreur_lambda[j] = np.std(erreur[:,j], ddof=0) # ddof = 0 to force the computation using the simple average\n",
    "print(erreur_lambda, std_erreur_lambda)\n",
    "\n",
    "# Relative error on the 6 training/test sets ?\n",
    "for p in range(6):\n",
    "    for lambda_p in range(lmax+1):\n",
    "        erreur_rel[p,lambda_p] = 100*np.abs(erreur[p,lambda_p] - erreur_lambda[lambda_p])/erreur_lambda[lambda_p]\n",
    "print(\"The relative error for each lambda value is \"+str(erreur_rel))\n",
    "\n",
    "\n",
    "# Averaged relative error on the 6 training/test sets ?\n",
    "erreur_rel_lambda=np.zeros(lmax+1)\n",
    "for n in range(lmax+1):\n",
    "    erreur_rel_lambda[n] = np.mean(erreur_rel[:,n])\n",
    "print(\"The averaged relative error for each lambda value over the 6 training/test sets is \"+str(erreur_rel_lambda))\n",
    "\n",
    "\n",
    "# standard variation of this error estimate ?\n",
    "\n",
    "# print(erreur_lambda, std_erreur_lambda, erreur_rel_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFlCAYAAADyLnFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28be4dfd4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exercise 4 FIGURE ---------------\n",
    "# ...\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(lambda_pos, erreur_lambda.T)\n",
    "\n",
    "ax.set(xlabel='lambda', ylabel='Mean Squared Error',\n",
    "       title='Estimated error as a function of lambda')\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(\"Lambda.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 4 (continued)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84961777545500838"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
