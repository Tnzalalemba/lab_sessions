{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 : Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this work is to implement least square linear regression to medical data. The problem is based on an example described in the book by Hastie & Tibshirani (2009) pp. 3-4 & 49-63. Data come from a study published by Stamey et al. (1989). This study aims at the prediction of the level of prostate specific antigen, denoted by `lpsa` below, from the\n",
    "results of clinical exams. These exams are carried out before a possible\n",
    "prostatectomy.\n",
    "\n",
    "The measurements are log cancer volume `lcavol`, log prostate \n",
    "weight `lweight`, age of the patient `age`, log of benign prostatic \n",
    "hyperplasia amount `lbph`, seminal vesicle invasion `svi`, log of capsular \n",
    "penetration `lcp`, Gleason score `gleason`, and percent of Gleason scores 4 or \n",
    "5 `pgg45`. The variables `svi` and `gleason` are categorical, others are\n",
    "quantitative. There are `p=8` entries.\n",
    "The work is decomposed in the following tasks:\n",
    "\n",
    "* read and format the data : extraction of the training and test sets,\n",
    "* apply least square regression method to predict `lpsa` from the entries,\n",
    "* study the estimated error on the test set (validation),\n",
    "* identify the most significant entries by using a rejection test,\n",
    "* apply regularized least square regression method (ridge regression),\n",
    "* search for an optimal regularization parameter thanks to\n",
    "cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "# import os\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Normalize data\n",
    "Data are stored in ASCII format: \n",
    "\n",
    "* the first column enumerates the data from 1 Ã  97 (97 male subjects). \n",
    "* columns 2 to 9 contain the entries themselves. \n",
    "* column 10 contains target values. \n",
    "* column 11 contains label 1 for the training set, \n",
    "and 2 for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% To read data from spaced separated float numbers\n",
    "# x, y = np.loadtxt(c, delimiter=',', usecols=(0, 2), unpack=True)\n",
    "\n",
    "data_init = np.loadtxt('prostate_data_sansheader.txt')\n",
    "\n",
    "data = data_init[:,1:]   # we get rid of the indices (1 to 97)\n",
    "\n",
    "#%% Extraction of training/test sets\n",
    "Itrain = np.nonzero(data[:,-1]==1)\n",
    "data_train=data[Itrain]   # original data\n",
    "\n",
    "Itest = np.nonzero(data[:,-1]==0)\n",
    "data_test = data[Itest]   # original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization of the data** *with respect to the mean and standard deviation of the training set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.23328245   0.47303067   7.44601122   1.45269103   0.41684299\n",
      "   1.39024269   0.70355366  29.08227243]\n"
     ]
    }
   ],
   "source": [
    "M_train = data_train\n",
    "M_test = data_test \n",
    "moy = np.zeros((8,))\n",
    "sigma = np.zeros((8,))\n",
    "\n",
    "# With a FOR loop :\n",
    "for k in range(8): # 8 columns of entries\n",
    "    moy[k]=np.mean(data_train[:,k])\n",
    "    sigma[k] = np.std(data_train[:,k], ddof=0)\n",
    "    M_train[:,k] = (data_train[:,k]-moy[k])/sigma[k] # normalized: centered, variance 1\n",
    "    M_test[:,k] = (data_test[:,k]-moy[k])/sigma[k]   # same normalization for test set\n",
    "\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Alternative WITHOUT FOR\\nnormalize = lambda vec: (vec-np.mean(vec))/np.std(vec)    # inline function \\nM_train = np.array( [ normalize(vec) for vec in data_train[:,0:8].T ] ).T  # iterate on vec direct / ARRAY not LIST\\nmoy = np.array( [ np.mean(vec) for vec in data_train[:,0:8].T ] )\\nsigma = np.array( [ np.std(vec, ddof=0) for vec in data_train[:,0:8].T ] )\\n\\nM_test = np.array([ (data_test[:,k]-moy[k])/sigma[k] for k in range(M_train.shape[1]) ] ).T\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Alternative WITHOUT FOR\n",
    "normalize = lambda vec: (vec-np.mean(vec))/np.std(vec)    # inline function \n",
    "M_train = np.array( [ normalize(vec) for vec in data_train[:,0:8].T ] ).T  # iterate on vec direct / ARRAY not LIST\n",
    "moy = np.array( [ np.mean(vec) for vec in data_train[:,0:8].T ] )\n",
    "sigma = np.array( [ np.std(vec, ddof=0) for vec in data_train[:,0:8].T ] )\n",
    "\n",
    "M_test = np.array([ (data_test[:,k]-moy[k])/sigma[k] for k in range(M_train.shape[1]) ] ).T\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : simple least square regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary questions\n",
    " \n",
    " * Compute the autocovariance matrix from the training set.\n",
    " * Observe carefully & Comment. What kind of information can you get ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.30023199  0.28632427  0.06316772  0.59294913  0.69204308\n",
      "   0.42641407  0.48316136]\n",
      " [ 0.30023199  1.          0.31672347  0.43704154  0.18105448  0.15682859\n",
      "   0.02355821  0.07416632]\n",
      " [ 0.28632427  0.31672347  1.          0.28734645  0.12890226  0.1729514\n",
      "   0.36591512  0.27580573]\n",
      " [ 0.06316772  0.43704154  0.28734645  1.         -0.1391468  -0.08853456\n",
      "   0.03299215 -0.03040382]\n",
      " [ 0.59294913  0.18105448  0.12890226 -0.1391468   1.          0.67124021\n",
      "   0.30687537  0.48135774]\n",
      " [ 0.69204308  0.15682859  0.1729514  -0.08853456  0.67124021  1.\n",
      "   0.47643684  0.66253335]\n",
      " [ 0.42641407  0.02355821  0.36591512  0.03299215  0.30687537  0.47643684\n",
      "   1.          0.7570565 ]\n",
      " [ 0.48316136  0.07416632  0.27580573 -0.03040382  0.48135774  0.66253335\n",
      "   0.7570565   1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe autocovariance matrix is symmetric.\\nWe can notice the the diagonal is giving us the varaiance (the square of the standard deviation) of features.\\nThe rest of the covariance matrix is telling about the covariance between the features.\\nThe covariances values are non-zero. Thus, the features are varying together.\\nWe can also notice that, since the features have been normalized, their variances (in the diagonal) are equal to 1, which is\\nthe square of their standard deviation.\\n\\n\\nThe 1st feature has a weak relationship with the 4th feature where the covariance is 0.063.\\nThe 2nd feature has a weak relationship with the 7th feature where the covariance is 0.023.\\nThe 4th feature has a weak relationship with the 8th feature where the covariance is-0.030,\\nand they are not varying in the same direction since their covariance is negative.\\n\\nThe 1st feature has a strong relationship with the 6th feature where the covariance is 0.692.\\nThe 5th feature has a strong relationship with the 6th feature where the covariance is 0.671.\\nThe 7th feature has a strong relationship with the 8th feature where the covariance is 0.757.\\nThe 8th feature has a strong relationship with the 6th feature where the covariance is 0.662.\\n\\nThe 4th and the 5th features are stongly varying in opposite directions since the record the lowest covariance value\\nwhich is -0.139\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preliminary questions\n",
    "\n",
    "# We are computing the autocovariance matrix of the trainning dataset containing the information about the features.\n",
    "cov_matrix = np.cov(M_train[:,0:8].T, ddof=0) # ddof = 0 to force the computation using the simple average\n",
    "\n",
    "print(cov_matrix)\n",
    "\n",
    "\n",
    "# Comments\n",
    "\"\"\"\n",
    "The autocovariance matrix is symmetric.\n",
    "We can notice the the diagonal is giving us the varaiance (the square of the standard deviation) of features.\n",
    "The rest of the covariance matrix is telling about the covariance between the features.\n",
    "The covariances values are non-zero. Thus, the features are varying together.\n",
    "We can also notice that, since the features have been normalized, their variances (in the diagonal) are equal to 1, which is\n",
    "the square of their standard deviation.\n",
    "\n",
    "\n",
    "The 1st feature has a weak relationship with the 4th feature where the covariance is 0.063.\n",
    "The 2nd feature has a weak relationship with the 7th feature where the covariance is 0.023.\n",
    "The 4th feature has a weak relationship with the 8th feature where the covariance is-0.030,\n",
    "and they are not varying in the same direction since their covariance is negative.\n",
    "\n",
    "The 1st feature has a strong relationship with the 6th feature where the covariance is 0.692.\n",
    "The 5th feature has a strong relationship with the 6th feature where the covariance is 0.671.\n",
    "The 7th feature has a strong relationship with the 8th feature where the covariance is 0.757.\n",
    "The 8th feature has a strong relationship with the 6th feature where the covariance is 0.662.\n",
    "\n",
    "The 4th and the 5th features are stongly varying in opposite directions since the record the lowest covariance value\n",
    "which is -0.139\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques rappels Ã  propos de la notion de covariance : https://fr.wikipedia.org/wiki/Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : least square regression \n",
    " * Build the matrix of features `X_train` for the training set, the first column is made of ones.\n",
    " * Estimate the regression vector `beta_hat` (estimates= `X*beta_hat`)\n",
    " _Indication: you may either use the function `inv` or another more efficient way to compute $A^{-1}B$ (think of `A\\B`)._ \n",
    " * What is the value of the first coefficient `beta_hat[0]` ? What does it correspond to ?\n",
    " * Estimate the prediction error (quadratic error) from the test set.\n",
    "\n",
    "\n",
    "*Indication: be careful of using `X_test` defined above, normalized w.r.t. the training data set. You can estimate this error by using:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_test = data_test[:,8]   # target column\n",
    "N_test = data_test.shape[0]\n",
    "X_test = np.concatenate((np.ones((N_test,1)), M_test[:,0:8]), axis=1) \n",
    "# don't forget the 1st column of ones and normalization !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "\n",
    "# Build the matrix of features X_train\n",
    "t_train = data_train[:,8]   # target column\n",
    "N_train = data_train.shape[0]\n",
    "X_train = np.concatenate((np.ones((N_train,1)), M_train[:,0:8]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression vector beta_hat is [ 2.45234509  0.71104059  0.29045029 -0.14148182  0.21041951  0.30730025\n",
      " -0.28684075 -0.02075686  0.27526843]\n"
     ]
    }
   ],
   "source": [
    "# Estimate the regression vector beta_hat (estimates= X*beta_hat)\n",
    "\n",
    "beta_hat =np.matmul(inv(np.matmul(X_train.T,(X_train))),np.matmul(X_train.T,t_train))\n",
    "print(\"The regression vector beta_hat is \"+str(beta_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the first coefficient of beta_hat is 2.45234508507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThis value corresponds to the intercept or the bias. It is a constant value that represent the mean value of the target.\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the value of the first coefficient beta_hat[0] ? What does it correspond to ?\n",
    "print(\"The value of the first coefficient of beta_hat is \"+str(beta_hat[0]))\n",
    "\n",
    "\"\"\"\n",
    "This value corresponds to the intercept or the bias. It is a constant value that represent the mean value of the target.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction error is 0.521274005508\n"
     ]
    }
   ],
   "source": [
    "# Estimate the prediction error (quadratic error) from the test set.\n",
    "estimates = np.matmul(X_test,beta_hat)\n",
    "\n",
    "# Computation of Mean Squared Error (MSE)\n",
    "MSE = np.mean(np.square(t_test - estimates))\n",
    "\n",
    "print(\"The prediction error is \"+str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection test, computation of Z-scores\n",
    "Now we turn to the selection of the most significant entries so that our predictor be more robust. The essential idea is that our estimates will be more robust if only the most significant entries are taken into account. As a consequence, note that we will *reduce the dimension* of the problem from |p=8| to some smaller dimension. The present approach uses a statistical test to decide whether the regression coefficient corresponding to some entry is significantly non-zero. Then we can decide either to put non significant coefficients to zero, or to select the significant entries only and estimate the new reduced regression vector.\n",
    "\n",
    "Let's assume that target values are noisy due to some white Gaussian\n",
    "noise with variance $\\sigma^2$ (see Hastie & Tibshirani p. 47). One can show that the estimated regression vector |beta_hat| is also Gaussian with variance\n",
    "\n",
    "$$ var (\\widehat{\\beta}) = (X^TX)^{-1}\\sigma^2.$$  \n",
    "\n",
    "One can also show that the estimator of the variance (from the training set)\n",
    "\n",
    "$$\\widehat{\\sigma^2}=\\frac{1}{(N-p-1)}\\sum (t_n-\\widehat{t}_n)^2$$\n",
    "\n",
    "obeys a Chi-2 distribution. As a consequence a Chi-square statistical test can be used to determine whether some coefficient $\\beta_j$ is\n",
    "significantly non-zero. To this aim, one defines the variables $z_j$\n",
    "named Z-scores which in turn obey a Fisher law, also called\n",
    "$t$-distribution, which are often used in statistics:\n",
    "\n",
    "$$ z_j = \\frac{\\beta_j}{\\widehat{\\sigma}\\sqrt{v_j}} $$\n",
    "\n",
    "where $v_j$ is the $j$-th diagonal element of the matrix $(X^TX)^{-1}$.\n",
    "For sake of simplicity, we will consider that the null hypothesis of\n",
    "$\\beta_j$ is rejected with probability 95% if the Z-score is greater than 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "1. Compute the Z-scores and select the most significant entries.\n",
    "2. Estimate the prediction error over the test set if only these significant \n",
    "entries are taken into account for regression by putting other regression \n",
    "coefficients to zero.\n",
    "3. Estimate the new regression vector when only the significant features\n",
    "are taken into account.\n",
    "4. Compare to previous results (Exercise 1).\n",
    "\n",
    "*Indication 1 : to sort a vector `Z` in descending order*\n",
    "`val = np.sort(np.abs(Z))[-1:0:-1]`\n",
    "\n",
    "\n",
    "*Indication 2 :* to extract the diagonal of a matrix,\n",
    "`vXX = np.diag(inv(X.T.dot(X),k=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# The variance of the target values 'Sigma_target^2'\\nSigma_target = np.std(t_train, ddof=0) # ddof = 0 to force the computation using the simple average\\n\\nvar_beta_hat = inv(np.matmul(X_train.T,X_train))*np.square(Sigma_target)\\n\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2\n",
    "\n",
    "# Compute the Z-scores and select the most significant entries.\n",
    "\"\"\"\n",
    "# The variance of the target values 'Sigma_target^2'\n",
    "Sigma_target = np.std(t_train, ddof=0) # ddof = 0 to force the computation using the simple average\n",
    "\n",
    "var_beta_hat = inv(np.matmul(X_train.T,X_train))*np.square(Sigma_target)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_hat_train = np.matmul(X_train,beta_hat) #Computed on the training set\n",
    "p = 8 # the number of features\n",
    "Sigma_square_hat = 1/(N_train-p-1)*np.sum(np.square(t_train-t_hat_train)) \n",
    "sigma_hat = np.sqrt(Sigma_square_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vXX = np.diag(inv(X_train.T.dot(X_train)),k=0) # Extraction of the diagonal of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_scores = beta_hat/(sigma_hat*np.sqrt(vXX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Z-score is given by [ 28.18152744   5.36629046   2.75078939   1.39590898   2.05584563\n",
      "   2.46925518   1.86691264   0.14668121   1.73783972]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTherefore, we can select the 1st, the 2nd, the 4th and the 5th features + the intercept for predictions.\\nBecause their Z-scores are higher than 2.\\nThis is a direct consequense of the following assumption: \"the null hypothesis of Beta_j is rejected with probanility of\\n95% if the Z-score is greater than 2.\" \\n\\nWith a confidence level of 95% we can affirm that there is no statistical significance to proof a relationship between features \\n3, 6, 7, 8 with respect to the target variable. Hence, they we can their B hats zero.\\n\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will consider that the null hypothesis of  Î²j  is rejected with probability 95% if the Z-score is greater than 2.\n",
    "print(\"The Z-score is given by \"+str(np.abs(Z_scores)))\n",
    "\n",
    "\"\"\"\n",
    "Therefore, we can select the 1st, the 2nd, the 4th and the 5th features + the intercept for predictions.\n",
    "Because their Z-scores are higher than 2.\n",
    "This is a direct consequense of the following assumption: \"the null hypothesis of Beta_j is rejected with probanility of\n",
    "95% if the Z-score is greater than 2.\" \n",
    "\n",
    "With a confidence level of 95% we can affirm that there is no statistical significance to proof a relationship between features \n",
    "3, 6, 7, 8 with respect to the target variable. Hence, they we can their B hats zero.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the prediction error over the test set if only these significant entries are taken into account for regression by putting other regression coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new regression vector beta_hat_new is [ 2.45234509  0.71104059  0.29045029  0.          0.21041951  0.30730025\n",
      "  0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# New beta_hat\n",
    "beta_hat_new = np.array([beta_hat[0], beta_hat[1], beta_hat[2], 0, beta_hat[4], beta_hat[5], 0, 0, 0])\n",
    "print(\"The new regression vector beta_hat_new is \"+str(beta_hat_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimate the new regression vector when only the significant features are taken into account.\n",
    "estimates_new = np.matmul(X_test,beta_hat_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new prediction error is 0.452226616073\n"
     ]
    }
   ],
   "source": [
    "# Computation of the new Mean Squared Error (MSE_new)\n",
    "MSE_new = np.mean(np.square(t_test - estimates_new))\n",
    "\n",
    "print(\"The new prediction error is \"+str(MSE_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Exercise 1, the MSE is 0.521274005508\n",
      "In Exercise 2, after features selection, the MSE is 0.452226616073, which is smaller than the one of Exercise 1\n",
      "Therefore, one can say that features selection with z-score improved the prediction error! And the predictor is more robust !\n"
     ]
    }
   ],
   "source": [
    "# Compare to previous results (Exercise 1).\n",
    "print(\"In the Exercise 1, the MSE is \"+str(MSE))\n",
    "print(\"In Exercise 2, after features selection, the MSE is \"+str(MSE_new)+\", which is smaller than the one of Exercise 1\")\n",
    "print(\"Therefore, one can say that features selection with z-score improved the prediction error! And the predictor is more robust !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regularized least squares\n",
    "This part deals with regularized least square regression. We denote\n",
    "by `beta_hat_reg` the resulting coefficients. This approach is an alternative to the selection based on statistical tests above. The idea is now to penalize large values of regression coefficients, *except for the bias*.\n",
    "\n",
    "We use the result:\n",
    "\n",
    "$$\\hat{\\beta} = (\\lambda I_p + X_c^T X_c)^{-1} X_c^T t_c$$\n",
    "\n",
    "where $X_c$ contains the normalized entries of the training data set with \n",
    "no column of ones (the bias should no be penalized and is processed). \n",
    "The targets `t_c` are therefore also centered, `t_c=t-mean(t)`.\n",
    " \n",
    "First, we estimate the bias $t_0$ to center the targets which yields the coefficient $\\beta_0$, that is `beta_hat_reg[0]` in Python.\n",
    "\n",
    "*Remark : the bias is estimated as the empirical average of targets.\n",
    "For tests, entries should be normalized with respect to the means and\n",
    "variances of the training data set (see exercise 3.5 p. 95 in Hastie & Tibshirani). Then work on the vector of entries with no column of ones.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "1. Use _ridge regression_ for penalty `lambda = 25` to estimate the regression vector. \n",
    "2. Estimate the prediction error from the test set.\n",
    "3. Compare the results (coefficients $\\beta$, error...) to previous ones.\n",
    "4. You may also compare these results to the result of best subset selection below:\n",
    "\n",
    "`beta_best = [2.477 0.74 0.316 0 0 0 0 0 0]`.\n",
    "\n",
    "*Indication : a simple way to obtain predictions for the test data set is the code below:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nt = data_train[:,8]   # column of targets\\nt0 = np.mean(t)\\n\\nN_test = data_test.shape[0]\\nX_test = np.hstack((np.ones((N_test,1)), M_test[:,0:8]))  \\n# Here the 1st column of X_test is a column of ones.\\nt_hat_reg = X_test.dot(beta_hat_reg)\\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "t = data_train[:,8]   # column of targets\n",
    "t0 = np.mean(t)\n",
    "\n",
    "N_test = data_test.shape[0]\n",
    "X_test = np.hstack((np.ones((N_test,1)), M_test[:,0:8]))  \n",
    "# Here the 1st column of X_test is a column of ones.\n",
    "t_hat_reg = X_test.dot(beta_hat_reg)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "# Center the target\n",
    "t = data_train[:,8]   # column of targets\n",
    "t0 = np.mean(t) # The mean of the targets. This value is the same as beta_hat[0]\n",
    "tc = t - t0 # Center the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the features: The features where already normalize within M_train\n",
    "Xc_train = M_train[:,0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lambda = 25 # The regularization term\n",
    "\n",
    "beta_hat_rg = inv(Lambda*np.identity(p) + Xc_train.T.dot(Xc_train)).dot(Xc_train.T).dot(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ridge regression vector is [ 2.45234509  0.4221092   0.24879171 -0.04226499  0.16575364  0.23091485\n",
      "  0.01066329  0.04306017  0.13151316]\n",
      "The regression vector of Exercise 1 is [ 2.45234509  0.71104059  0.29045029 -0.14148182  0.21041951  0.30730025\n",
      " -0.28684075 -0.02075686  0.27526843]\n",
      "The regression vector of Exercise 2, after features selection, is [ 2.45234509  0.71104059  0.29045029  0.          0.21041951  0.30730025\n",
      "  0.          0.          0.        ]\n",
      "The main common point between the above regression vectors is the intercept/the bias which is the same !\n",
      "We can notice as well that some coefficients increased and other decreased.\n",
      "We can notice that even if we are using different estimators, the beta_hats from both of the ridge regression andthe simple least square regression maintain some consistency regarding which features are the least important.\n"
     ]
    }
   ],
   "source": [
    "# The ridge regression vector is therefore:\n",
    "\n",
    "beta_hat_reg = np.hstack((beta_hat[0], beta_hat_rg[0:8]))\n",
    "\n",
    "print(\"The ridge regression vector is \"+str(beta_hat_reg))\n",
    "print(\"The regression vector of Exercise 1 is \"+str(beta_hat))\n",
    "print(\"The regression vector of Exercise 2, after features selection, is \"+str(beta_hat_new))\n",
    "\n",
    "print(\"The main common point between the above regression vectors is the intercept/the bias which is the same !\")\n",
    "print(\"We can notice as well that some coefficients increased and other decreased.\")\n",
    "print(\"We can notice that even if we are using different estimators, the beta_hats from both of the ridge regression and\"+\n",
    "      \"the simple least square regression maintain some consistency regarding which features are the least important.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction error in ridge regression is 0.493942159912.\n",
      "The error in ridge regression is lower than in regular regression (Exercise 1), and higher than the one after features selection in Exercise 2.\n"
     ]
    }
   ],
   "source": [
    "# Estimate the prediction error from the test set.\n",
    "\n",
    "# Estimation of the prediction over test data\n",
    "N_test = data_test.shape[0]\n",
    "Xc_test = np.hstack((np.ones((N_test,1)), M_test[:,0:8])) # From M_test, the test data are already normalized.\n",
    "\n",
    "t_hat_reg_test = X_test.dot(beta_hat_reg)\n",
    "\n",
    "# Computation of the Mean Squared Error in ridge regression (MSE_reg)\n",
    "MSE_reg = np.mean(np.square(t_test - t_hat_reg_test))\n",
    "\n",
    "print(\"The prediction error in ridge regression is \"+str(MSE_reg)+\".\")\n",
    "print(\"The error in ridge regression is lower than in regular regression (Exercise 1), and higher than the one after features selection in Exercise 2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction error with the best regression vector is 0.493942159912.\n"
     ]
    }
   ],
   "source": [
    "# Estimation of the prediction error over test data with the best regression vector\n",
    "beta_best = np.array([ 2.477,  0.74 ,  0.316, 0.0,  0.0, 0.0,  0.0,  0.0,  0.0])\n",
    "\n",
    "t_hat_best_test = Xc_test.dot(beta_hat_reg)\n",
    "MSE_best = np.mean(np.square(t_test - t_hat_best_test))\n",
    "\n",
    "print(\"The prediction error with the best regression vector is \"+str(MSE_best)+\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_best-MSE_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears that the prediction with the best regession vector gives the same mean squared error as the ridge regression vector.\n"
     ]
    }
   ],
   "source": [
    "# You may also compare these results to the result of best subset selection\n",
    "print(\"It appears that the prediction with the best regession vector gives the same mean squared error as the ridge regression vector.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Cross-Validation \n",
    "\n",
    "## How to choose lambda from the training data set only ? \n",
    "\n",
    "The idea is to decompose the training set in 2 subsets: one subset for\n",
    "linear regression (say 9/10 of the data), the other to estimate the prediction error (say 1/10 of the data).\n",
    "\n",
    "We can repeat this operation 10 times over the 10 possible couples of\n",
    "subsets to estimate the average prediction error. We will choose the\n",
    "value of `lambda` which minimizes this error. The algorithm goes as\n",
    "follows:\n",
    "\n",
    "For the 10 cross-validation cases\n",
    "    \n",
    "    Extraction of test & training subsets `testset` & `trainset`\n",
    "    \n",
    "    For lambda in 0:40\n",
    "        Estimate `beta_hat` from normalized `trainset` (mean=0, var=1)\n",
    "        Estimate the error from  `testset`\n",
    "    EndFor lambda\n",
    "\n",
    "EndFor 10 cases\n",
    "\n",
    "Compute the average error for each lambda\n",
    "\n",
    "Choose `lambda` which minimizes the error \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "* Use 6-fold cross-validation in the present study to optimize the choice of `lambda`. \n",
    "Try values of `lambda` ranging from 0 to 40 for instance (0:40).\n",
    "* Plot the estimated error as a function of `lambda`.\n",
    "* Propose a well chosen value of `lambda` and give the estimated corresponding\n",
    "error on the test set.\n",
    "* Comment on your results.\n",
    "\n",
    "*Indication 1 : think of shuffling the dataset first.*\n",
    "\n",
    "*Indication 2 : you can build 6 training and test subsets by using the code below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmax = 40\n",
    "lambda_pos = arange(0,lmax+1) \n",
    "\n",
    "N_test = 10\n",
    "m=np.zeros(8)\n",
    "s = np.zeros(8)\n",
    "X_traink = np.zeros((X_train.shape[0]-N_test,8))\n",
    "X_testk = np.zeros((N_test,8))\n",
    "erreur = np.zeros((6,lmax+1))\n",
    "erreur_rel = np.zeros((6,lmax+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.04820039  1.07555701  1.10176217  1.1269283   1.15115733  1.17453933\n",
      "   1.19715304  1.219067    1.24034084  1.26102657  1.28116963  1.30080991\n",
      "   1.31998256  1.33871867  1.35704586  1.37498878  1.39256952  1.40980794\n",
      "   1.42672197  1.44332786  1.45964039  1.47567306  1.49143822  1.50694722\n",
      "   1.52221049  1.53723772  1.55203782  1.56661913  1.58098936  1.59515575\n",
      "   1.60912503  1.62290355  1.63649724  1.64991171  1.66315222  1.67622375\n",
      "   1.68913103  1.70187851  1.71447044  1.72691084  1.73920355]\n",
      " [ 0.32536477  0.30984217  0.29763909  0.28786152  0.27990441  0.27334495\n",
      "   0.2678796   0.26328534  0.25939497  0.25608096  0.25324447  0.25080785\n",
      "   0.24870933  0.24689923  0.24533713  0.24398991  0.24283014  0.24183492\n",
      "   0.24098504  0.24026422  0.23965858  0.23915626  0.23874701  0.23842196\n",
      "   0.23817338  0.2379945   0.23787937  0.23782274  0.23781996  0.23786686\n",
      "   0.23795974  0.23809529  0.23827049  0.23848265  0.23872931  0.23900825\n",
      "   0.23931744  0.23965502  0.24001929  0.24040868  0.24082176]\n",
      " [ 1.15294263  1.07997368  1.02861321  0.98956094  0.95800017  0.93126624\n",
      "   0.90780509  0.8866698   0.86726133  0.84918789  0.83218533  0.81607038\n",
      "   0.8007123   0.78601529  0.77190721  0.7583323   0.74524633  0.73261334\n",
      "   0.7204034   0.70859104  0.69715422  0.68607347  0.6753314   0.66491227\n",
      "   0.65480166  0.64498627  0.63545377  0.6261926   0.61719195  0.60844162\n",
      "   0.59993197  0.59165387  0.58359866  0.57575811  0.56812438  0.56068999\n",
      "   0.55344784  0.5463911   0.53951328  0.53280818  0.52626984]\n",
      " [ 0.51278895  0.51467329  0.51641738  0.51793267  0.51918293  0.52015921\n",
      "   0.52086673  0.52131792  0.52152856  0.52151567  0.52129629  0.52088695\n",
      "   0.52030325  0.51955978  0.51867008  0.51764662  0.51650088  0.51524338\n",
      "   0.5138838   0.51243101  0.51089314  0.50927767  0.50759145  0.50584079\n",
      "   0.50403151  0.50216894  0.500258    0.49830324  0.49630886  0.49427873\n",
      "   0.49221643  0.49012529  0.48800836  0.4858685   0.48370836  0.48153036\n",
      "   0.4793368   0.47712978  0.47491126  0.47268307  0.4704469 ]\n",
      " [ 0.47635253  0.45661931  0.43962279  0.42473319  0.41151006  0.39963489\n",
      "   0.38887079  0.37903726  0.36999391  0.36162956  0.35385477  0.34659658\n",
      "   0.33979477  0.33339903  0.32736699  0.32166257  0.31625486  0.31111717\n",
      "   0.30622627  0.30156185  0.29710605  0.29284305  0.28875883  0.28484084\n",
      "   0.28107784  0.27745972  0.27397735  0.27062245  0.2673875   0.26426563\n",
      "   0.26125056  0.25833654  0.25551828  0.25279089  0.25014988  0.24759108\n",
      "   0.2451106   0.24270487  0.24037053  0.23810445  0.23590373]\n",
      " [ 0.51807756  0.50381195  0.49241785  0.48326046  0.47586871  0.46988754\n",
      "   0.46504556  0.4611327   0.45798443  0.45547054  0.45348695  0.45194973\n",
      "   0.45079064  0.44995374  0.4493929   0.44906976  0.44895227  0.44901348\n",
      "   0.4492306   0.44958427  0.45005794  0.45063742  0.45131049  0.45206657\n",
      "   0.45289648  0.4537922   0.45474671  0.45575388  0.45680827  0.45790509\n",
      "   0.45904011  0.46020956  0.46141006  0.46263861  0.46389253  0.4651694\n",
      "   0.46646705  0.46778351  0.469117    0.47046593  0.47182884]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4 \n",
    "\n",
    "# Shuffling the dataset first\n",
    "# import random\n",
    "# random.shuffle(data_train) # Randomly shuffle the arrays of the dataset\n",
    "\n",
    "for p in range(6):   # loop on test subsets\n",
    "    # extraction of testset\n",
    "    testset  = data_train[arange(p*N_test,(p+1)*N_test),0:9] \n",
    "    # extraction of trainset\n",
    "    trainset = data_train[hstack((arange(p*N_test),arange((p+1)*N_test,data_train.shape[0]))),0:9]\n",
    "    # Center the target\n",
    "    target_train = trainset[:,8]  # column of targets\n",
    "    target0 = np.mean(target_train) # The mean of the targets.\n",
    "    targetc = target_train - target0 # The targets of the train set are centered\n",
    "    # normalization of entries, features\n",
    "    for i in range(8): # 8 columns of entries\n",
    "        m[i]=np.mean(trainset[:,i])\n",
    "        s[i] = np.std(trainset[:,i], ddof=0)\n",
    "        X_traink[:,i] = (trainset[:,i]-m[i])/s[i] # normalized: centered, variance 1\n",
    "        X_testk[:,i] = (testset[:,i]-m[i])/s[i]   # same normalization for test set\n",
    "    for lambda_p in range(lmax+1): # 40 options of lambda\n",
    "        beta_hat_cv = inv(lambda_p*np.identity(8) + X_traink.T.dot(X_traink)).dot(X_traink.T).dot(targetc)\n",
    "        X_test_cv = np.hstack((np.ones((N_test,1)), X_testk[:,0:8]))\n",
    "        \n",
    "        beta_hat_reg_cv = np.hstack((beta_hat[0], beta_hat_cv[0:8]))\n",
    "        t_hat_cv_test = X_test_cv.dot(beta_hat_reg_cv)\n",
    "        # Computation of the Mean Squared Error in ridge regression (erreur)\n",
    "        erreur[p,lambda_p] = np.mean(np.square(testset[:,8] - t_hat_cv_test))\n",
    "print(erreur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6722878   0.65674624  0.64607875  0.63837951  0.63260394  0.62813869\n",
      "  0.62460347  0.62175167  0.61941734  0.6174852   0.61587291  0.61452023\n",
      "  0.61338214  0.61242429  0.61162003  0.61094832  0.61039233  0.60993837\n",
      "  0.60957518  0.60929337  0.60908505  0.60894349  0.6088629   0.60883828\n",
      "  0.60886523  0.60893989  0.60905884  0.60921901  0.60941765  0.60965228\n",
      "  0.60992064  0.61022068  0.61055051  0.61090841  0.61129278  0.61170214\n",
      "  0.61213513  0.61259046  0.61306697  0.61356353  0.6140791 ] [ 0.3109947   0.30509309  0.30507868  0.30797329  0.31238065  0.31760446\n",
      "  0.32327955  0.32920637  0.33527193  0.34141023  0.34758155  0.35376125\n",
      "  0.35993349  0.36608777  0.37221677  0.37831523  0.38437917  0.39040549\n",
      "  0.39639169  0.4023357   0.40823577  0.41409044  0.41989846  0.42565878\n",
      "  0.43137051  0.43703292  0.4426454   0.4482075   0.45371884  0.45917919\n",
      "  0.46458837  0.46994633  0.47525306  0.48050866  0.48571324  0.49086703\n",
      "  0.49597026  0.50102323  0.50602628  0.51097977  0.51588411]\n",
      "The relative error for each lambda value is [[  55.9154253    63.77056364   70.53063166   76.52952142   81.97125626\n",
      "    86.98725967   91.66608897   96.06975862  100.24315751  104.21972452\n",
      "   108.02500237  111.67893898  115.19742254  118.59333247  121.87727603\n",
      "   125.05811523  128.14335092  131.13940752  134.05184719  136.88553331\n",
      "   139.64475676  142.33333464  144.95468821  147.51190518  150.00778975\n",
      "   152.44490335  154.82559794  157.15204353  159.42625099  161.6500912\n",
      "   163.82531114  165.95354749  168.03633832  170.075133    172.07130082\n",
      "   174.02613838  175.94087612  177.81668394  179.65467617  181.45591599\n",
      "   183.2214193 ]\n",
      " [  51.60335062   52.82163036   53.93145362   54.90746256   55.7536088\n",
      "    56.4833441    57.11205316   57.65426125   58.12274632   58.52840527\n",
      "    58.88040138   59.18639693   59.45279184   59.68493907   59.88732851\n",
      "    60.06373985   60.21736793   60.35092493   60.46672341   60.5667439\n",
      "    60.65268998   60.72603328   60.78805037   60.83985314   60.88241384\n",
      "    60.91658593   60.94312135   60.96268499   60.97586685   60.9831922\n",
      "    60.9851303    60.98210162   60.97448416   60.96261869   60.94681336\n",
      "    60.92734755   60.90447531   60.87842823   60.84941794   60.81763836\n",
      "    60.78326758]\n",
      " [  71.4953952    64.44307112   59.2086431    55.01138695   51.43759234\n",
      "    48.2580598    45.34102583   42.60835025   40.01243873   37.5236024\n",
      "    35.12289985   32.79796737   30.54053093   28.34489144   26.2069869\n",
      "    24.1238037    22.09300316   20.11268265   18.1812221    16.29718514\n",
      "    14.45925556   12.66619671   10.91682584    9.20999822    7.54459756\n",
      "     5.91953055    4.33372402    2.7861237     1.275694      0.19858169\n",
      "     1.63770038    3.0426388     4.41435267    5.75377608    7.06182096\n",
      "     8.33937668    9.58730975   10.80646368   11.99765891   13.16169286\n",
      "    14.29934011]\n",
      " [  23.72478756   21.6328524    20.0689729    18.86759271   17.9292283\n",
      "    17.19038945   16.608416     16.15335425   15.80336416   15.54199727\n",
      "    15.35651502   15.23681125   15.17469931   15.16342648   15.19733492\n",
      "    15.2716191    15.3821489    15.52533795   15.69804396   15.89749214\n",
      "    16.12121537   16.36700699   16.63288289   16.91705084   17.21788536\n",
      "    17.53390696   17.86376479   18.20622197   18.56014323   18.9244843\n",
      "    19.29828269   19.68064983   20.07076409   20.46786475   20.87124662\n",
      "    21.28025528   21.69428288   22.11276435   22.535174     22.96102249\n",
      "    23.38985404]\n",
      " [  29.14455271   30.47248887   31.955231     33.46697656   34.94981026\n",
      "    36.37792118   37.74117283   39.03719386   40.26742723   41.43510413\n",
      "    42.54418947   43.59883274   44.60308746   45.56077564   46.47543063\n",
      "    47.3502823    48.1882643    48.99203231   49.76398685   50.50629749\n",
      "    51.22092637   51.90965024   52.57408061   53.21568189   53.83578752\n",
      "    54.43561435   55.01627525   55.57879026   56.12409648   56.65305662\n",
      "    57.16646673   57.66506284   58.14952695   58.62049224   59.07854773\n",
      "    59.52424244   59.95808899   60.38056691   60.79212552   61.19318657\n",
      "    61.58414653]\n",
      " [  22.93812961   23.28666313   23.78361724   24.29887654   24.77620123\n",
      "    25.19366473   25.54547281   25.83329951   26.06205853   26.23782025\n",
      "    26.36679635   26.45486544   26.50737486   26.52908272   26.52416888\n",
      "    26.49627769   26.44857294   26.38379498   26.30431507   26.21218492\n",
      "    26.1091806    25.99684083   25.87650018   25.74931754   25.61630059\n",
      "    25.47832665   25.33616058   25.19047001   25.04183844   24.89077639\n",
      "    24.73773103   24.58309439   24.42721044   24.27038124   24.11287215\n",
      "    23.95491643   23.79671918   23.63846077   23.4802998    23.32237571\n",
      "    23.16481104]]\n",
      "The averaged relative error for each lambda value over the 6 training/test sets is [ 42.4702735   42.73787825  43.24642492  43.84696946  44.4696162\n",
      "  45.08177316  45.66903827  46.22603629  46.75186541  47.24777564\n",
      "  47.71596741  48.15896879  48.57931782  48.97940797  49.36142098\n",
      "  49.72730631  50.07878469  50.41736339  50.74435643  51.06090615\n",
      "  51.36800411  51.66651045  51.95717135  52.24063447  52.51746244\n",
      "  52.78814463  53.05310732  53.31272241  53.567315    53.88336373\n",
      "  54.60843705  55.31784916  56.01211277  56.691711    57.35710027\n",
      "  58.00871279  58.64695871  59.27222798  59.88489206  60.48530533\n",
      "  61.07380643]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4 ---------------\n",
    "# ...\n",
    "# Averaged error on the 6 training/test sets on each lambda ?\n",
    "erreur_lambda=np.zeros(lmax+1)\n",
    "std_erreur_lambda=np.zeros(lmax+1)\n",
    "\n",
    "for j in range(lmax+1):\n",
    "    erreur_lambda[j] = np.mean(erreur[:,j])\n",
    "    std_erreur_lambda[j] = np.std(erreur[:,j], ddof=0) # ddof = 0 to force the computation using the simple average\n",
    "print(erreur_lambda, std_erreur_lambda)\n",
    "\n",
    "# Relative error on the 6 training/test sets ?\n",
    "for p in range(6):\n",
    "    for lambda_p in range(lmax+1):\n",
    "        erreur_rel[p,lambda_p] = 100*np.abs(erreur[p,lambda_p] - erreur_lambda[lambda_p])/erreur_lambda[lambda_p]\n",
    "print(\"The relative error for each lambda value is \"+str(erreur_rel))\n",
    "\n",
    "\n",
    "# Averaged relative error on the 6 training/test sets ?\n",
    "erreur_rel_lambda=np.zeros(lmax+1)\n",
    "for n in range(lmax+1):\n",
    "    erreur_rel_lambda[n] = np.mean(erreur_rel[:,n])\n",
    "print(\"The averaged relative error for each lambda value over the 6 training/test sets is \"+str(erreur_rel_lambda))\n",
    "\n",
    "\n",
    "# standard variation of this error estimate ?\n",
    "\n",
    "# print(erreur_lambda, std_erreur_lambda, erreur_rel_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFlCAYAAADyLnFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ/JeEg9Zl0kLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21263863c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exercise 4 FIGURE ---------------\n",
    "# ...\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(lambda_pos, erreur_lambda.T)\n",
    "\n",
    "ax.set(xlabel='lambda', ylabel='Mean Squared Error',\n",
    "       title='Estimated error as a function of lambda')\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(\"Lambda.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 (continued)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
